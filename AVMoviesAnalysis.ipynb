{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import glob\n",
    "import random\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from enum import Enum\n",
    "from subprocess import call\n",
    "from copy import copy\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import threading\n",
    "\n",
    "\n",
    "PATH_TO_PROJECT = os.getcwd()\n",
    "PATH_TO_DATA = \"/home/tomasz/Dokumenty/shared/\"\n",
    "DATASET_FOLDER = \"data\"\n",
    "EXTRACTED_DATA_FOLDER = \"extracted_data\"\n",
    "DATASET_RESULT_FILE = \"ACCEDEranking.txt\"\n",
    "VIDO_EXTENSION = \".mp4\"\n",
    "PATH_TO_DATASET = os.path.join(PATH_TO_DATA, DATASET_FOLDER)\n",
    "VIDEO_CLASSES = [\"Neutral\", \"LALV\", \"LAHV\", \"HALV\", \"HAHV\"]\n",
    "MIN_NEUTRAL_LEVEL_VALUE = 3600\n",
    "MAX_NEUTRAL_LEVEL_VALUE = 6300\n",
    "SPLIT_LEVEL = 5000\n",
    "\n",
    "FRAME_SIZE = 100\n",
    "MAX_FRAMES = 300\n",
    "\n",
    "class ValueLevel(Enum):\n",
    "    Low = 1\n",
    "    LowN = 2\n",
    "    HighN = 3\n",
    "    High = 4\n",
    "    \n",
    "class VideoClass(Enum):\n",
    "    Neutral = 0\n",
    "    LALV = 1\n",
    "    LAHV = 2\n",
    "    HALV = 3\n",
    "    HAHV = 4\n",
    "      \n",
    "\n",
    "class threadsafe_iterator:\n",
    "    def __init__(self, iterator):\n",
    "        self.iterator = iterator\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        with self.lock:\n",
    "            return next(self.iterator)\n",
    "\n",
    "def threadsafe_generator(func):\n",
    "    \"\"\"Decorator\"\"\"\n",
    "    @functools.wraps(func)\n",
    "    def gen(*a, **kw):\n",
    "        return threadsafe_iterator(func(*a, **kw))\n",
    "    return gen\n",
    "    \n",
    "\n",
    "class DataSet():\n",
    "    def __init__(self, seq_length=40, image_shape=(224, 224, 3)):\n",
    "        \n",
    "        \"\"\"Constructor.\n",
    "        seq_length = (int) the number of frames to consider\n",
    "        \"\"\"\n",
    "        self.path_to_dataset = os.path.join(PATH_TO_DATA, DATASET_FOLDER)\n",
    "        self.path_to_extracted_data = os.path.join(PATH_TO_DATA, EXTRACTED_DATA_FOLDER)\n",
    "        self.seq_length = seq_length\n",
    "        self.max_frames = MAX_FRAMES  # max number of frames a video can have for us to use it\n",
    "        # Get the data.\n",
    "        self.data = self.get_data()\n",
    "\n",
    "        # Now do some minor data cleaning.\n",
    "        self.data = self.clean_data()\n",
    "\n",
    "        self.image_shape = image_shape\n",
    "        \n",
    "    def get_data(self):\n",
    "        data = []\n",
    "        path_to_result = os.path.join(PATH_TO_DATA, DATASET_RESULT_FILE)\n",
    "        list_of_results = open(path_to_result).readlines()\n",
    "        list_of_results.pop(0)\n",
    "        for res in list_of_results:\n",
    "            splitted_res = res.split(\"\\t\")\n",
    "            video_name = splitted_res[1]\n",
    "            video_name = video_name[:-4]\n",
    "            valency = splitted_res[2]\n",
    "            arousal = splitted_res[3]\n",
    "            emotion_class = DataSet.get_class_for_arousal_and_valency(int(arousal), int(valency))\n",
    "            number_of_frames = self.extract_data_for_video(video_name)\n",
    "            #print(video_name + \": \" + arousal + \", \" + valency + \", \" + emotion_class.name + \", \" + str(number_of_frames))\n",
    "            data.append([video_name, int(valency), int(arousal), emotion_class, number_of_frames])\n",
    "\n",
    "        return data\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_class_for_arousal_and_valency(arousal, valency):\n",
    "        if arousal < MIN_NEUTRAL_LEVEL_VALUE:\n",
    "            if valency < SPLIT_LEVEL:\n",
    "                return VideoClass.LALV\n",
    "            else:\n",
    "                return VideoClass.LAHV\n",
    "        elif arousal < SPLIT_LEVEL:\n",
    "            if valency < MIN_NEUTRAL_LEVEL_VALUE:\n",
    "                return VideoClass.LALV\n",
    "            elif valency < MAX_NEUTRAL_LEVEL_VALUE:\n",
    "                return VideoClass.Neutral\n",
    "            else:\n",
    "                return VideoClass.LAHV\n",
    "        elif arousal < MAX_NEUTRAL_LEVEL_VALUE:\n",
    "            if valency < MIN_NEUTRAL_LEVEL_VALUE:\n",
    "                return VideoClass.HALV\n",
    "            elif valency < MAX_NEUTRAL_LEVEL_VALUE:\n",
    "                return VideoClass.Neutral\n",
    "            else:\n",
    "                return VideoClass.HAHV\n",
    "        else:\n",
    "            if valency < SPLIT_LEVEL:\n",
    "                return VideoClass.HALV\n",
    "            else:\n",
    "                return VideoClass.HAHV\n",
    "            \n",
    "    def extract_data_for_video(self, video_name):\n",
    "        if not os.path.isdir(self.path_to_extracted_data):\n",
    "            os.mkdir(self.path_to_extracted_data)\n",
    "        if not os.path.isdir(os.path.join(self.path_to_extracted_data, video_name)):\n",
    "            os.mkdir(os.path.join(self.path_to_extracted_data, video_name))\n",
    "            \n",
    "        if not self.check_if_video_is_extracted(video_name):\n",
    "            src = os.path.join(self.path_to_dataset, video_name + VIDO_EXTENSION)\n",
    "            dest = os.path.join(self.path_to_extracted_data, video_name,\n",
    "                        '%04d.jpg')\n",
    "            call([\"ffmpeg\", \"-i\", src, dest])\n",
    "        return len(self.get_frames_for_video(video_name))\n",
    "            \n",
    "    def get_frames_for_video(self, video_name):\n",
    "        images = sorted(glob.glob(os.path.join(self.path_to_extracted_data, video_name, '*jpg')))\n",
    "        return images\n",
    "        \n",
    "    def check_if_video_is_extracted(self, video_name):\n",
    "        return bool(os.path.exists(os.path.join(self.path_to_extracted_data, video_name,\n",
    "                               '0001.jpg')))\n",
    "        \n",
    "    def clean_data(self):\n",
    "        \"\"\"Limit samples to greater than the sequence length and fewer\n",
    "        than N frames. Also limit it to classes we want to use.\"\"\"\n",
    "        data_clean = []\n",
    "        for item in self.data:\n",
    "            if int(item[4]) >= self.seq_length and int(item[4]) <= self.max_frames:\n",
    "                data_clean.append(item)\n",
    "\n",
    "        return data_clean\n",
    "    \n",
    "    def split_train_test(self, percent_of_train):\n",
    "        number_of_train = int(len(self.data) * percent_of_train)\n",
    "        y = copy(self.data)\n",
    "        random.shuffle(y)\n",
    "        train = y[:number_of_train]\n",
    "        test = y[number_of_train:]\n",
    "        \n",
    "        return train, test\n",
    "    \n",
    "    def build_image_sequence(self, frames):\n",
    "        \"\"\"Given a set of frames (filenames), build our sequence.\"\"\"\n",
    "        return [self.process_image(x, self.image_shape) for x in frames]\n",
    "    \n",
    "    def process_image(self, image, target_shape):\n",
    "        \"\"\"Given an image, process it and return the array.\"\"\"\n",
    "        # Load the image.\n",
    "        h, w, _ = target_shape\n",
    "        image = load_img(image, target_size=(h, w))\n",
    "\n",
    "        # Turn it into numpy, normalize and return.\n",
    "        img_arr = img_to_array(image)\n",
    "        x = (img_arr / 255.).astype(np.float32)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def get_all_sequences_in_memory(self, train_or_test, precent_of_train):\n",
    "        train, test = self.split_train_test(precent_of_train)\n",
    "        data = train if train_or_test == \"train\" else test\n",
    "        \n",
    "        X, y = [], []\n",
    "        \n",
    "        for row in data:\n",
    "            frames = self.get_frames_for_video(row[0])\n",
    "            frames = DataSet.rescale_list(frames, self.seq_length)\n",
    "            sequence = self.build_image_sequence(frames)\n",
    "            X.append(sequence)\n",
    "            y.append(self.get_class_one_hot(row[3]))\n",
    "            \n",
    "        return np.array(X), np.array(y)\n",
    "        \n",
    "    @threadsafe_generator\n",
    "    def frame_generator(self, batch_size, train_test, precent_of_train):\n",
    "        train, test = self.split_train_test(precent_of_train)\n",
    "        data = train if train_or_test == \"train\" else test\n",
    "        \n",
    "        while 1:\n",
    "            X, y = [], []\n",
    "            \n",
    "            for _ in range(batch_size):\n",
    "                sample = random.choice(data)\n",
    "                frames = self.get_frames_for_video(sample[0])\n",
    "                frames = DataSet.rescale_list(frames, self.seq_length)\n",
    "                sequence = self.build_image_sequence(frames)\n",
    "                X.append(sequence)\n",
    "                y.append(self.get_class_one_hot(sample[3]))\n",
    "            \n",
    "            \n",
    "            yield np.array(X), np.array(y)\n",
    "            \n",
    "    def get_class_one_hot(self, video_class):\n",
    "        # Now one-hot it.\n",
    "        label_hot = to_categorical(video_class.value, len(VideoClass))\n",
    "        return label_hot\n",
    "            \n",
    "    @staticmethod        \n",
    "    def rescale_list(input_list, size):\n",
    "        \"\"\"Given a list and a size, return a rescaled/samples list. For example,\n",
    "        if we want a list of size 5 and we have a list of size 25, return a new\n",
    "        list of size five which is every 5th element of the origina list.\"\"\"\n",
    "        assert len(input_list) >= size\n",
    "\n",
    "        # Get the number to skip between iterations.\n",
    "        skip = len(input_list) // size\n",
    "\n",
    "        # Build our new output.\n",
    "        output = [input_list[i] for i in range(0, len(input_list), skip)]\n",
    "\n",
    "        # Cut off the last one if needed.\n",
    "        return output[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, ZeroPadding3D, Input, Activation\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling3D, Conv3D, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow import metrics\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import sys\n",
    "\n",
    "class AVAnalysisModel():   \n",
    "    def __init__(self, seq_length, saved_model=None):\n",
    "        self.seq_length = seq_length\n",
    "        self.input_shape = (seq_length, 80, 80, 3)\n",
    "        \n",
    "        if saved_model is not None:\n",
    "            self.model = load_model(self.saved_model)\n",
    "        else:\n",
    "            self.model = self.lrcn()\n",
    "            \n",
    "        # Now compile the network.\n",
    "        optimizer = Adam(lr=1e-5, decay=1e-6)\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    def lrcn(self):\n",
    "        \"\"\"Build a CNN into RNN.\n",
    "        Starting version from:\n",
    "            https://github.com/udacity/self-driving-car/blob/master/\n",
    "                steering-models/community-models/chauffeur/models.py\n",
    "        Heavily influenced by VGG-16:\n",
    "            https://arxiv.org/abs/1409.1556\n",
    "        Also known as an LRCN:\n",
    "            https://arxiv.org/pdf/1411.4389.pdf\n",
    "        \"\"\"\n",
    "        def add_default_block(model, kernel_filters, init, reg_lambda):\n",
    "\n",
    "            # conv\n",
    "            model.add(TimeDistributed(Conv2D(kernel_filters, (3, 3), padding='same',\n",
    "                                             kernel_initializer=init, kernel_regularizer=l2(l=reg_lambda))))\n",
    "            model.add(TimeDistributed(tf.compat.v1.layers.BatchNormalization()))\n",
    "            model.add(TimeDistributed(Activation('relu')))\n",
    "            # conv\n",
    "            model.add(TimeDistributed(Conv2D(kernel_filters, (3, 3), padding='same',\n",
    "                                             kernel_initializer=init, kernel_regularizer=l2(l=reg_lambda))))\n",
    "            model.add(TimeDistributed(tf.compat.v1.layers.BatchNormalization()))\n",
    "            model.add(TimeDistributed(Activation('relu')))\n",
    "            # max pool\n",
    "            model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "            return model\n",
    "\n",
    "        initialiser = 'glorot_uniform'\n",
    "        reg_lambda  = 0.001\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        # first (non-default) block\n",
    "        model.add(TimeDistributed(Conv2D(32, (7, 7), strides=(2, 2), padding='same',\n",
    "                                         kernel_initializer=initialiser, kernel_regularizer=l2(l=reg_lambda)),\n",
    "                                  input_shape=self.input_shape))\n",
    "        model.add(TimeDistributed(tf.compat.v1.layers.BatchNormalization()))\n",
    "        model.add(TimeDistributed(Activation('relu')))\n",
    "        model.add(TimeDistributed(Conv2D(32, (3,3), kernel_initializer=initialiser, kernel_regularizer=l2(l=reg_lambda))))\n",
    "        model.add(TimeDistributed(tf.compat.v1.layers.BatchNormalization()))\n",
    "        model.add(TimeDistributed(Activation('relu')))\n",
    "        model.add(TimeDistributed(MaxPooling2D((2, 2), strides=(2, 2))))\n",
    "\n",
    "        # 2nd-5th (default) blocks\n",
    "        model = add_default_block(model, 64,  init=initialiser, reg_lambda=reg_lambda)\n",
    "        model = add_default_block(model, 128, init=initialiser, reg_lambda=reg_lambda)\n",
    "        model = add_default_block(model, 256, init=initialiser, reg_lambda=reg_lambda)\n",
    "        model = add_default_block(model, 512, init=initialiser, reg_lambda=reg_lambda)\n",
    "\n",
    "        # LSTM output head\n",
    "        model.add(TimeDistributed(Flatten()))\n",
    "        model.add(LSTM(256, return_sequences=False, dropout=0.5))\n",
    "        model.add(Dense(len(VideoClass), activation='softmax'))\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os.path\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "\n",
    "PERCENT_OF_TRAIN = 0.8\n",
    "\n",
    "def train(seq_length, saved_model=None, image_shape=None,\n",
    "          load_to_memory=False, batch_size=32, nb_epoch=100, loaded_data=None):\n",
    "    \n",
    "    # Helper: TensorBoard\n",
    "    tb = TensorBoard(log_dir=os.path.join('data', 'logs'))\n",
    "\n",
    "    # Helper: Stop when we stop learning.\n",
    "    early_stopper = EarlyStopping(patience=5)\n",
    "\n",
    "    # Get the data and process it.\n",
    "    if loaded_data is not None:\n",
    "        data = loaded_data\n",
    "    elif image_shape is None:\n",
    "        data = DataSet(\n",
    "            seq_length=seq_length\n",
    "        )\n",
    "    else:\n",
    "        data = DataSet(\n",
    "            seq_length=seq_length,\n",
    "            image_shape=image_shape\n",
    "        )\n",
    "\n",
    "    # Get samples per epoch.\n",
    "    # Multiply by 0.7 to attempt to guess how much of data.data is the train set.\n",
    "    steps_per_epoch = (len(data.data) * 0.7) // batch_size\n",
    "\n",
    "    if load_to_memory:\n",
    "        # Get data.\n",
    "        X, y = data.get_all_sequences_in_memory('train', PERCENT_OF_TRAIN)\n",
    "        X_test, y_test = data.get_all_sequences_in_memory('test', PERCENT_OF_TRAIN)\n",
    "    else:\n",
    "        # Get generators.\n",
    "        generator = data.frame_generator(batch_size, 'train', PERCENT_OF_TRAIN)\n",
    "        val_generator = data.frame_generator(batch_size, 'test', PERCENT_OF_TRAIN)\n",
    "\n",
    "    # Get the model.\n",
    "    rm = AVAnalysisModel(seq_length, saved_model)\n",
    "    print(X.shape)\n",
    "    \n",
    "\n",
    "    # Fit!\n",
    "    if load_to_memory:\n",
    "        # Use standard fit.\n",
    "        rm.model.fit(\n",
    "            X,\n",
    "            y,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(X_test, y_test),\n",
    "            #verbose=1,\n",
    "            #callbacks=[tb, early_stopper],\n",
    "            epochs=nb_epoch)\n",
    "    else:\n",
    "        # Use fit generator.\n",
    "        rm.model.fit_generator(\n",
    "            generator=generator,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            epochs=nb_epoch,\n",
    "            verbose=1,\n",
    "            callbacks=[tb, early_stopper],\n",
    "            validation_data=val_generator,\n",
    "            validation_steps=40,\n",
    "            workers=4)\n",
    " \n",
    "    now = time.strftime(\"%c\")\n",
    "    rm.save(str(now) + '.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = None  # None or weights file\n",
    "seq_length = 40\n",
    "load_to_memory = True  # pre-load the sequences into memory\n",
    "batch_size = 16\n",
    "nb_epoch = 10\n",
    "image_shape = (80, 80, 3)\n",
    "data = DataSet(\n",
    "            seq_length=seq_length,\n",
    "            image_shape=image_shape\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31, 40, 80, 80, 3)\n",
      "Train on 31 samples, validate on 8 samples\n",
      "Epoch 1/10\n",
      "16/31 [==============>...............] - ETA: 4s"
     ]
    },
    {
     "ename": "_SymbolicException",
     "evalue": "Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'time_distributed_37/batch_normalization/cond_1/Identity:0' shape=() dtype=float32>, <tf.Tensor 'time_distributed_37/batch_normalization/cond/Identity_1:0' shape=(32,) dtype=float32>, <tf.Tensor 'time_distributed_37/batch_normalization/cond/Identity_2:0' shape=(32,) dtype=float32>, <tf.Tensor 'time_distributed_40/batch_normalization/cond_1/Identity:0' shape=() dtype=float32>, <tf.Tensor 'time_distributed_40/batch_normalization/cond/Identity_1:0' shape=(32,) dtype=float32>, <tf.Tensor 'time_distributed_40/batch_normalization/cond/Identity_2:0' shape=(32,) dtype=float32>, <tf.Tensor 'time_distributed_44/batch_normalization/cond_1/Identity:0' shape=() dtype=float32>, <tf.Tensor 'time_distributed_44/batch_normalization/cond/Identity_1:0' shape=(64,) dtype=float32>, <tf.Tensor 'time_distributed_44/batch_normalization/cond/Identity_2:0' shape=(64,) dtype=float32>, <tf.Tensor 'time_distributed_47/batch_normalization/cond_1/Identity:0' shape=() dtype=float32>, <tf.Tensor 'time_distributed_47/batch_normalization/cond/Identity_1:0' shape=(64,) dtype=float32>, <tf.Tensor 'time_distributed_47/batch_normalization/cond/Identity_2:0' shape=(64,) dtype=float32>, <tf.Tensor 'time_distributed_51/batch_normalization/cond_1/Identity:0' shape=() dtype=float32>, <tf.Tensor 'time_distributed_51/batch_normalization/cond/Identity_1:0' shape=(128,) dtype=float32>, <tf.Tensor 'time_distributed_51/batch_normalization/cond/Identity_2:0' shape=(128,) dtype=float32>, <tf.Tensor 'time_distributed_54/batch_normalization/cond_1/Identity:0' shape=() dtype=float32>, <tf.Tensor 'time_distributed_54/batch_normalization/cond/Identity_1:0' shape=(128,) dtype=float32>, <tf.Tensor 'time_distributed_54/batch_normalization/cond/Identity_2:0' shape=(128,) dtype=float32>, <tf.Tensor 'time_distributed_58/batch_normalization/cond_1/Identity:0' shape=() dtype=float32>, <tf.Tensor 'time_distributed_58/batch_normalization/cond/Identity_1:0' shape=(256,) dtype=float32>, <tf.Tensor 'time_distributed_58/batch_normalization/cond/Identity_2:0' shape=(256,) dtype=float32>, <tf.Tensor 'time_distributed_61/batch_normalization/cond_1/Identity:0' shape=() dtype=float32>, <tf.Tensor 'time_distributed_61/batch_normalization/cond/Identity_1:0' shape=(256,) dtype=float32>, <tf.Tensor 'time_distributed_61/batch_normalization/cond/Identity_2:0' shape=(256,) dtype=float32>, <tf.Tensor 'time_distributed_65/batch_normalization/cond_1/Identity:0' shape=() dtype=float32>, <tf.Tensor 'time_distributed_65/batch_normalization/cond/Identity_1:0' shape=(512,) dtype=float32>, <tf.Tensor 'time_distributed_65/batch_normalization/cond/Identity_2:0' shape=(512,) dtype=float32>, <tf.Tensor 'time_distributed_68/batch_normalization/cond_1/Identity:0' shape=() dtype=float32>, <tf.Tensor 'time_distributed_68/batch_normalization/cond/Identity_1:0' shape=(512,) dtype=float32>, <tf.Tensor 'time_distributed_68/batch_normalization/cond/Identity_2:0' shape=(512,) dtype=float32>]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: An op outside of the function building code is being passed\na \"Graph\" tensor. It is possible to have Graph tensors\nleak out of the function building context by including a\ntf.init_scope in your function building code.\nFor example, the following function will fail:\n  @tf.function\n  def has_init_scope():\n    my_constant = tf.constant(1.)\n    with tf.init_scope():\n      added = my_constant * 2\nThe graph tensor has name: time_distributed_37/batch_normalization/cond_1/Identity:0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31m_SymbolicException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-da9579c09568>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m train(seq_length, saved_model=saved_model, image_shape=image_shape,\n\u001b[0;32m----> 2\u001b[0;31m           load_to_memory=load_to_memory, batch_size=batch_size, nb_epoch=nb_epoch, loaded_data=data)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-455cf1e14c73>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(seq_length, saved_model, image_shape, load_to_memory, batch_size, nb_epoch, loaded_data)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;31m#verbose=1,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;31m#callbacks=[tb, early_stopper],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             epochs=nb_epoch)\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# Use fit generator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     73\u001b[0m       raise core._SymbolicException(\n\u001b[1;32m     74\u001b[0m           \u001b[0;34m\"Inputs to eager execution function cannot be Keras symbolic \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m           \"tensors, but found {}\".format(keras_symbolic_tensors))\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_SymbolicException\u001b[0m: Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'time_distributed_37/batch_normalization/cond_1/Identity:0' shape=() dtype=float32>, <tf.Tensor 'time_distributed_37/batch_normalization/cond/Identity_1:0' shape=(32,) dtype=float32>, <tf.Tensor 'time_distributed_37/batch_normalization/cond/Identity_2:0' shape=(32,) dtype=float32>, <tf.Tensor 'time_distributed_40/batch_normalization/cond_1/Identity:0' shape=() dtype=float32>, <tf.Tensor 'time_distributed_40/batch_normalization/cond/Identity_1:0' shape=(32,) dtype=float32>, <tf.Tensor 'time_distributed_40/batch_normalization/cond/Identity_2:0' shape=(32,) dtype=float32>, <tf.Tensor 'time_distributed_44/batch_normalization/cond_1/Identity:0' shape=() dtype=float32>, <tf.Tensor 'time_distributed_44/batch_normalization/cond/Identity_1:0' shape=(64,) dtype=float32>, <tf.Tensor 'time_distributed_44/batch_normalization/cond/Identity_2:0' shape=(64,) dtype=float32>, <tf.Tensor 'time_distributed_47/batch_normalization/cond_1/Identity:0' shape=() dtype=float32>, <tf.Tensor 'time_distributed_47/batch_normalization/cond/Identity_1:0' shape=(64,) dtype=float32>, <tf.Tensor 'time_distributed_47/batch_normalization/cond/Identity_2:0' shape=(64,) dtype=float32>, <tf.Tensor 'time_distributed_51/batch_normalization/cond_1/Identity:0' shape=() dtype=float32>, <tf.Tensor 'time_distributed_51/batch_normalization/cond/Identity_1:0' shape=(128,) dtype=float32>, <tf.Tensor 'time_distributed_51/batch_normalization/cond/Identity_2:0' shape=(128,) dtype=float32>, <tf.Tensor 'time_distributed_54/batch_normalization/cond_1/Identity:0' shape=() dtype=float32>, <tf.Tensor 'time_distributed_54/batch_normalization/cond/Identity_1:0' shape=(128,) dtype=float32>, <tf.Tensor 'time_distributed_54/batch_normalization/cond/Identity_2:0' shape=(128,) dtype=float32>, <tf.Tensor 'time_distributed_58/batch_normalization/cond_1/Identity:0' shape=() dtype=float32>, <tf.Tensor 'time_distributed_58/batch_normalization/cond/Identity_1:0' shape=(256,) dtype=float32>, <tf.Tensor 'time_distributed_58/batch_normalization/cond/Identity_2:0' shape=(256,) dtype=float32>, <tf.Tensor 'time_distributed_61/batch_normalization/cond_1/Identity:0' shape=() dtype=float32>, <tf.Tensor 'time_distributed_61/batch_normalization/cond/Identity_1:0' shape=(256,) dtype=float32>, <tf.Tensor 'time_distributed_61/batch_normalization/cond/Identity_2:0' shape=(256,) dtype=float32>, <tf.Tensor 'time_distributed_65/batch_normalization/cond_1/Identity:0' shape=() dtype=float32>, <tf.Tensor 'time_distributed_65/batch_normalization/cond/Identity_1:0' shape=(512,) dtype=float32>, <tf.Tensor 'time_distributed_65/batch_normalization/cond/Identity_2:0' shape=(512,) dtype=float32>, <tf.Tensor 'time_distributed_68/batch_normalization/cond_1/Identity:0' shape=() dtype=float32>, <tf.Tensor 'time_distributed_68/batch_normalization/cond/Identity_1:0' shape=(512,) dtype=float32>, <tf.Tensor 'time_distributed_68/batch_normalization/cond/Identity_2:0' shape=(512,) dtype=float32>]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train(seq_length, saved_model=saved_model, image_shape=image_shape,\n",
    "          load_to_memory=load_to_memory, batch_size=batch_size, nb_epoch=nb_epoch, loaded_data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
